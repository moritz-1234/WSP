{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9677383",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits\n",
    "\n",
    "In this notebook we'll explore the MAB framework and different methods to address the exploration-exploitation trade-off:\n",
    "- Understand the MAB problem and how to formulate it as a reinforcement learning problem\n",
    "- Implement and compare exploration strategies: random, (mostly) greedy, epsilon-greedy, softmax, and UCB\n",
    "- Observe the trade-off between exploration and exploitation by comparing reward and regret of all strategies\n",
    "\n",
    "The only libraries you'll need are NumPy and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5352e",
   "metadata": {},
   "source": [
    "### 1. Create the multi-armed Bandit \n",
    "- Complete the following MAB class (marked as TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f7b8e",
   "metadata": {},
   "source": [
    "### 2. Understand the environment\n",
    "- Create a bandit environment with 8 arms\n",
    "- Print the optimal arm and the highest reward\n",
    "- Plot the mean and standard deviation of all arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533fdfd",
   "metadata": {},
   "source": [
    "### 3. Implement different exploration strategies\n",
    "\n",
    "To compare the different strategies set up a function for each of the below mentioned strategies. Each function:\n",
    "- inputs an instance of the bandit, the number of steps to run (i.e. arm pulls) as well as strategy-specific parameters, e.g. epsilon\n",
    "- runs for 1000 steps (default) and with each step chooses an action (depending on the strategy) and updates *N*, *Q* and the *cumulative rewards* for each step \n",
    "- updates the estimated reward Q after each pull using incremental averaging: $Q_{t+1}(a)= Q_{t}(a) + \\frac{1}{N(a)} (R-Q_t(a))$\n",
    "- returns: \n",
    "  - the *cumulated rewards* for all steps, \n",
    "  - *N*: how often each arm was called\n",
    "\n",
    "*Mostly Greedy*\n",
    "- Choose each arm once\n",
    "- For the remaining runs, choose greedily, i.e. the arm with the highest action value Q \n",
    "\n",
    "*Epsilon-Greedy*   \n",
    "- With a probability of 1-$\\epsilon$ choose the arm with the highest estimated reward\n",
    "- With a probability of $\\epsilon$ choose a random arm\n",
    "- Typical $\\epsilon$-value: 0.1\n",
    "  \n",
    "*Softmax*\n",
    "- Calculate a probability distribution over the action values by converting the estimated rewards into probabilities: $P(a) = \\frac{e^{Q(a)/\\tau}}{\\sum_i e^{Q(i)/\\tau}}$\n",
    "- In each run, sample an arm according to this probability distribution\n",
    "- Typical $\\tau$-value: 0.1\n",
    "\n",
    "*Upper Confidence Bound*\n",
    "- In each run, select the arm that maximises $Q(a) + c \\sqrt{\\frac{\\text{ln}\\: t}{N(a)}}$\n",
    "- Typical $c$-value: 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d8ed7",
   "metadata": {},
   "source": [
    "### 4. Create a Function to Run Experiments\n",
    "\n",
    "Set up a function called `experiment` that inputs:\n",
    "- the number of bandit arms `k`\n",
    "- the number of arm pulls each strategy performs, i.e. `steps` within the strategy function\n",
    "- the number of `runs` to perform for the experiment, i.e. each strategy is called 200 times\n",
    "\n",
    "Within the function:\n",
    "- Set a seed, e.g. using a random number generator, to create repeatable experiments\n",
    "- Set up variables for the `rewards` and `counts` of each strategy\n",
    "- `Regret`: Create an empty list to store the true means of each bandit you create\n",
    "- Run a loop for `runs` times\n",
    "  - Every round, set up a bandit with a new seed. You can use the random number generator and `randint` to create a repeatable seed\n",
    "  - Store the true means of the arms\n",
    "  - Run the strategies and stor rewards and counts\n",
    "- Now you have gathered the rewards and counts for 200 runs, where in each run each strategy has pulled its arm 1000 times\n",
    "- Calculate the average cumulative reward over the 200 runs for every strategy\n",
    "- Calculate the average regret: `Regret(t) = t * optimal reward - avg. cumulative reward`.    \n",
    "  Tip: For `t`, you might use: `t_array = np.arange(1, steps + 1)  `, which is similar to range(1, steps+1) but as an array\n",
    "\n",
    "Return:\n",
    "- `Average cumulative rewards`, `Average regret` and `t_array` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67131eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fdebb",
   "metadata": {},
   "source": [
    "### 5. Running the Experiment and Plotting \n",
    "\n",
    "After setting up the `experiment` function\n",
    "- Run the function and plot the results for all strategies\n",
    "- Try out different hyperparameters (epsilon, tau, c) \n",
    "- You can also change the range out of which the arm rewards are chosen or the number of arms\n",
    "- Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wps2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
