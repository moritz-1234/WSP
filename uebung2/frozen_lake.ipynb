{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "The frozen lake environment is a stochastic grid cell environment, with finite episodes. You can find the Gymnasium documentation [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "Write your own frozen lake agent or complete the following code:\n",
    "- Calculate the q-values using the Bellman Optimality Equation\n",
    "- Determine the optimal policy\n",
    "\n",
    "Interpretation:\n",
    "- Print your learning progress (q-values, v-values, learned policy)\n",
    "- How fast does your agent learn the optimal policy?\n",
    "\n",
    "Test your code and experiment with different hyperparameters, e.g.\n",
    "- Discount factor\n",
    "- Exploration strategy (e.g., epsilon-greedy with different epsilon values)\n",
    "- Map size (e.g., 4x4, 8x8). How does the map size affect learning and performance?\n",
    "- Is the training performance comparable to the test performance? Why (not)?\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FrozenLakeEnv<FrozenLake-v1>>\n",
      "{0: {0: [(0.33333333333333337, 0, 0, False), (0.3333333333333333, 0, 0, False), (0.33333333333333337, 4, 0, False)], 1: [(0.33333333333333337, 0, 0, False), (0.3333333333333333, 4, 0, False), (0.33333333333333337, 1, 0, False)], 2: [(0.33333333333333337, 4, 0, False), (0.3333333333333333, 1, 0, False), (0.33333333333333337, 0, 0, False)], 3: [(0.33333333333333337, 1, 0, False), (0.3333333333333333, 0, 0, False), (0.33333333333333337, 0, 0, False)]}, 1: {0: [(0.33333333333333337, 1, 0, False), (0.3333333333333333, 0, 0, False), (0.33333333333333337, 5, 0, True)], 1: [(0.33333333333333337, 0, 0, False), (0.3333333333333333, 5, 0, True), (0.33333333333333337, 2, 0, False)], 2: [(0.33333333333333337, 5, 0, True), (0.3333333333333333, 2, 0, False), (0.33333333333333337, 1, 0, False)], 3: [(0.33333333333333337, 2, 0, False), (0.3333333333333333, 1, 0, False), (0.33333333333333337, 0, 0, False)]}, 2: {0: [(0.33333333333333337, 2, 0, False), (0.3333333333333333, 1, 0, False), (0.33333333333333337, 6, 0, False)], 1: [(0.33333333333333337, 1, 0, False), (0.3333333333333333, 6, 0, False), (0.33333333333333337, 3, 0, False)], 2: [(0.33333333333333337, 6, 0, False), (0.3333333333333333, 3, 0, False), (0.33333333333333337, 2, 0, False)], 3: [(0.33333333333333337, 3, 0, False), (0.3333333333333333, 2, 0, False), (0.33333333333333337, 1, 0, False)]}, 3: {0: [(0.33333333333333337, 3, 0, False), (0.3333333333333333, 2, 0, False), (0.33333333333333337, 7, 0, True)], 1: [(0.33333333333333337, 2, 0, False), (0.3333333333333333, 7, 0, True), (0.33333333333333337, 3, 0, False)], 2: [(0.33333333333333337, 7, 0, True), (0.3333333333333333, 3, 0, False), (0.33333333333333337, 3, 0, False)], 3: [(0.33333333333333337, 3, 0, False), (0.3333333333333333, 3, 0, False), (0.33333333333333337, 2, 0, False)]}, 4: {0: [(0.33333333333333337, 0, 0, False), (0.3333333333333333, 4, 0, False), (0.33333333333333337, 8, 0, False)], 1: [(0.33333333333333337, 4, 0, False), (0.3333333333333333, 8, 0, False), (0.33333333333333337, 5, 0, True)], 2: [(0.33333333333333337, 8, 0, False), (0.3333333333333333, 5, 0, True), (0.33333333333333337, 0, 0, False)], 3: [(0.33333333333333337, 5, 0, True), (0.3333333333333333, 0, 0, False), (0.33333333333333337, 4, 0, False)]}, 5: {0: [(1.0, 5, 0, True)], 1: [(1.0, 5, 0, True)], 2: [(1.0, 5, 0, True)], 3: [(1.0, 5, 0, True)]}, 6: {0: [(0.33333333333333337, 2, 0, False), (0.3333333333333333, 5, 0, True), (0.33333333333333337, 10, 0, False)], 1: [(0.33333333333333337, 5, 0, True), (0.3333333333333333, 10, 0, False), (0.33333333333333337, 7, 0, True)], 2: [(0.33333333333333337, 10, 0, False), (0.3333333333333333, 7, 0, True), (0.33333333333333337, 2, 0, False)], 3: [(0.33333333333333337, 7, 0, True), (0.3333333333333333, 2, 0, False), (0.33333333333333337, 5, 0, True)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(0.33333333333333337, 4, 0, False), (0.3333333333333333, 8, 0, False), (0.33333333333333337, 12, 0, True)], 1: [(0.33333333333333337, 8, 0, False), (0.3333333333333333, 12, 0, True), (0.33333333333333337, 9, 0, False)], 2: [(0.33333333333333337, 12, 0, True), (0.3333333333333333, 9, 0, False), (0.33333333333333337, 4, 0, False)], 3: [(0.33333333333333337, 9, 0, False), (0.3333333333333333, 4, 0, False), (0.33333333333333337, 8, 0, False)]}, 9: {0: [(0.33333333333333337, 5, 0, True), (0.3333333333333333, 8, 0, False), (0.33333333333333337, 13, 0, False)], 1: [(0.33333333333333337, 8, 0, False), (0.3333333333333333, 13, 0, False), (0.33333333333333337, 10, 0, False)], 2: [(0.33333333333333337, 13, 0, False), (0.3333333333333333, 10, 0, False), (0.33333333333333337, 5, 0, True)], 3: [(0.33333333333333337, 10, 0, False), (0.3333333333333333, 5, 0, True), (0.33333333333333337, 8, 0, False)]}, 10: {0: [(0.33333333333333337, 6, 0, False), (0.3333333333333333, 9, 0, False), (0.33333333333333337, 14, 0, False)], 1: [(0.33333333333333337, 9, 0, False), (0.3333333333333333, 14, 0, False), (0.33333333333333337, 11, 0, True)], 2: [(0.33333333333333337, 14, 0, False), (0.3333333333333333, 11, 0, True), (0.33333333333333337, 6, 0, False)], 3: [(0.33333333333333337, 11, 0, True), (0.3333333333333333, 6, 0, False), (0.33333333333333337, 9, 0, False)]}, 11: {0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}, 12: {0: [(1.0, 12, 0, True)], 1: [(1.0, 12, 0, True)], 2: [(1.0, 12, 0, True)], 3: [(1.0, 12, 0, True)]}, 13: {0: [(0.33333333333333337, 9, 0, False), (0.3333333333333333, 12, 0, True), (0.33333333333333337, 13, 0, False)], 1: [(0.33333333333333337, 12, 0, True), (0.3333333333333333, 13, 0, False), (0.33333333333333337, 14, 0, False)], 2: [(0.33333333333333337, 13, 0, False), (0.3333333333333333, 14, 0, False), (0.33333333333333337, 9, 0, False)], 3: [(0.33333333333333337, 14, 0, False), (0.3333333333333333, 9, 0, False), (0.33333333333333337, 12, 0, True)]}, 14: {0: [(0.33333333333333337, 10, 0, False), (0.3333333333333333, 13, 0, False), (0.33333333333333337, 14, 0, False)], 1: [(0.33333333333333337, 13, 0, False), (0.3333333333333333, 14, 0, False), (0.33333333333333337, 15, 1, True)], 2: [(0.33333333333333337, 14, 0, False), (0.3333333333333333, 15, 1, True), (0.33333333333333337, 10, 0, False)], 3: [(0.33333333333333337, 15, 1, True), (0.3333333333333333, 10, 0, False), (0.33333333333333337, 13, 0, False)]}, 15: {0: [(1.0, 15, 0, True)], 1: [(1.0, 15, 0, True)], 2: [(1.0, 15, 0, True)], 3: [(1.0, 15, 0, True)]}}\n",
      "[0.53006594 0.48303111 0.45230256 0.43713666 0.54783225 0.\n",
      " 0.35040218 0.         0.58314889 0.63686767 0.6097876  0.\n",
      " 0.         0.73744109 0.86067745 0.        ]\n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment, with slippery cells and an 4x4 map\n",
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\", is_slippery=True)\n",
    "env=env.unwrapped\n",
    "# get environment details\n",
    "num_states = env.observation_space.n  # 16 cells\n",
    "num_actions = env.action_space.n  # 4 actions\n",
    "print(env)\n",
    "P = env.P  # transition probabilities: {state: [(trans. prob., next state, reward, done), ...]}\n",
    "print(P)\n",
    "# hyperparameters\n",
    "gamma = 0.99  # discount factor\n",
    "theta = 1e-3  # convergence threshold\n",
    "\n",
    "# initialise the value function with all zeros\n",
    "V = np.zeros(num_states)\n",
    "# initialise the policy with all zeros\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "while True:\n",
    "    delta = 0  # to track convergence\n",
    "\n",
    "    # loop over all states\n",
    "    for state in range(num_states):\n",
    "        ### calculate the q-values ###\n",
    "        v = V[state]\n",
    "        vals = np.zeros(len(P[state].items()))\n",
    "        vals[:] = -np.inf\n",
    "        for idx, p in P[state].items(): #different action possible from state state\n",
    "            val =0\n",
    "            for pp in p: #possible outcomes of choosing an action\n",
    "                prob = pp[0]\n",
    "                next_state = pp[1]\n",
    "                reward=pp[2]\n",
    "                done = pp[3]\n",
    "                val += prob*(reward+gamma*V[next_state])\n",
    "            vals[idx]=val\n",
    "        V[state]=np.max(vals)\n",
    "        # update the delta\n",
    "        delta = max(delta, abs(v - V[state]))\n",
    "    \n",
    "    # check for convergence\n",
    "    if delta < theta:\n",
    "        break\n",
    "print(V)\n",
    "\n",
    "### determine the optimal policy ###\n",
    "for state in range(num_states):\n",
    "    vals = np.zeros(len(P[state].items()))\n",
    "    vals[:] = -np.inf\n",
    "    for idx, p in P[state].items():\n",
    "        val =0\n",
    "        for pp in p:\n",
    "            prob = pp[0]\n",
    "            next_state = pp[1]\n",
    "            reward=pp[2]\n",
    "            done = pp[3]\n",
    "            val += prob*(reward+gamma*V[next_state])\n",
    "        vals[idx]=val\n",
    "    policy[state]=np.argmax(vals)\n",
    "\n",
    "print(policy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "Success Rate: 84.00%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the policy over a number of episodes\n",
    "def evaluate_policy(env, policy, episodes=100):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        observation, info = env.reset()\n",
    "        episode_over = 0\n",
    "        total_reward =  0\n",
    "\n",
    "        while not episode_over:\n",
    "            # Take the action and see what happens\n",
    "            action = policy[observation]\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if(truncated):\n",
    "                episode_over=1\n",
    "            if(terminated):\n",
    "                episode_over=2\n",
    "        if(episode_over==2 and total_reward == 1):\n",
    "            success+=1\n",
    "    return success / episodes\n",
    "\n",
    "\n",
    "def pretty_print_map(map):\n",
    "    print('\\nmap: ')\n",
    "    # get the map from the environment (which is a numpy array of byte strings, i.e. b'S')\n",
    "    # convert the byte array to a string array\n",
    "    map_str = np.char.decode(map, 'utf-8')\n",
    "    # pretty print the map\n",
    "    for row in map_str:\n",
    "        print(' '.join(row))  # removes brackets etc.\n",
    "    print(end='\\n')\n",
    "\n",
    "def pretty_print_policy(policy):\n",
    "    policy_directions = {0:'left', 1:'down', 2:'right', 3:'up'}\n",
    "    print('directions: ')\n",
    "    for direction in policy:\n",
    "        print(policy_directions[direction], end=' ')\n",
    "    print(end='\\n\\n')\n",
    "\n",
    "# env.unwrapped.desc gives the map as byte strings\n",
    "success_rate = evaluate_policy(env, policy,100)\n",
    "pretty_print_map(env.unwrapped.desc)  \n",
    "pretty_print_policy(policy)\n",
    "print(f'Success Rate: {success_rate * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frozen_lake:\n",
    "    def __init__(self,gamma =0.99, theta=1e-3,episodes = 100, epsilon = 0 ,seed = 1):\n",
    "        self.gamma=gamma\n",
    "        self.theta=theta\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.rng = np.random\n",
    "        self.rng.seed(seed)\n",
    "        env = gym.make('FrozenLake-v1',map_name=\"4x4\", is_slippery=True)\n",
    "        self.env=env.unwrapped\n",
    "        # get environment details\n",
    "        num_states = self.env.observation_space.n  # 16 cells\n",
    "        self.num_actions = self.env.action_space.n  # 4 actions\n",
    "        P = self.env.P  # transition probabilities: {state: [(trans. prob., next state, reward, done), ...]}\n",
    "\n",
    "        # initialise the value function with all zeros\n",
    "        V = np.zeros(num_states)\n",
    "        # initialise the policy with all zeros\n",
    "        policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "        # Value Iteration Algorithm\n",
    "        while True:\n",
    "            delta = 0  # to track convergence\n",
    "\n",
    "            # loop over all states\n",
    "            for state in range(num_states):\n",
    "                ### calculate the q-values ###\n",
    "                v = V[state]\n",
    "                vals = np.zeros(len(P[state].items()))\n",
    "                vals[:] = -np.inf\n",
    "                for idx, p in P[state].items(): #different action possible from state state\n",
    "                    val =0\n",
    "                    for pp in p: #possible outcomes of choosing an action\n",
    "                        prob = pp[0]\n",
    "                        next_state = pp[1]\n",
    "                        reward=pp[2]\n",
    "                        done = pp[3]\n",
    "                        val += prob*(reward+gamma*V[next_state])\n",
    "                    vals[idx]=val\n",
    "                V[state]=np.max(vals)\n",
    "                # update the delta\n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "            \n",
    "            # check for convergence\n",
    "            if delta < theta:\n",
    "                break\n",
    "        ### determine the optimal policy ###\n",
    "        for state in range(num_states):\n",
    "            vals = np.zeros(len(P[state].items()))\n",
    "            vals[:] = -np.inf\n",
    "            for idx, p in P[state].items():\n",
    "                val =0\n",
    "                for pp in p:\n",
    "                    prob = pp[0]\n",
    "                    next_state = pp[1]\n",
    "                    reward=pp[2]\n",
    "                    done = pp[3]\n",
    "                    val += prob*(reward+gamma*V[next_state])\n",
    "                vals[idx]=val\n",
    "            policy[state]=np.argmax(vals)\n",
    "        self.policy=policy\n",
    "    def pretty_print_map(self):\n",
    "        print('\\nmap: ')\n",
    "        map = self.env.unwrapped.desc\n",
    "        # get the map from the environment (which is a numpy array of byte strings, i.e. b'S')\n",
    "        # convert the byte array to a string array\n",
    "        map_str = np.char.decode(map, 'utf-8')\n",
    "        # pretty print the map\n",
    "        for row in map_str:\n",
    "            print(' '.join(row))  # removes brackets etc.\n",
    "        print(end='\\n')\n",
    "\n",
    "    def pretty_print_policy(self):\n",
    "        policy_directions = {0:'left', 1:'down', 2:'right', 3:'up'}\n",
    "        print('directions: ')\n",
    "        for direction in self.policy:\n",
    "            print(policy_directions[direction], end=' ')\n",
    "        print(end='\\n\\n')\n",
    "    def evaluate_policy(self):\n",
    "        success = 0\n",
    "        for _ in range(self.episodes):\n",
    "            observation, info = self.env.reset()\n",
    "            episode_over = 0\n",
    "            total_reward =  0\n",
    "            while not episode_over:\n",
    "                # Take the action and see what happens\n",
    "                action = self.policy[observation]\n",
    "                rnd = self.rng.random()\n",
    "                if(rnd <self.epsilon): #epsilon greedy\n",
    "                    action = self.rng.randint(0,self.num_actions)\n",
    "                observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if(truncated):\n",
    "                    episode_over=1\n",
    "                if(terminated):\n",
    "                    episode_over=2\n",
    "            if(episode_over==2 and total_reward == 1):\n",
    "                success+=1\n",
    "        return success / self.episodes\n",
    "    def run(self):\n",
    "        success_rate = self.evaluate_policy()\n",
    "        self.pretty_print_map()  \n",
    "        self.pretty_print_policy()\n",
    "        print(\"-------\")\n",
    "        print(f\"Hyperparameters: gamma={self.gamma}, theta={self.theta}, episodes={self.episodes}, epsilon={self.epsilon}\")\n",
    "        print(f'Success Rate: {success_rate * 100:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=0.001, episodes=100, epsilon=0\n",
      "Success Rate: 84.00%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=0.001, episodes=100, epsilon=0.1\n",
      "Success Rate: 49.00%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=0.001, episodes=1000, epsilon=0.1\n",
      "Success Rate: 41.90%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=0.001, episodes=1000, epsilon=0.2\n",
      "Success Rate: 24.40%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=1e-05, episodes=100, epsilon=0\n",
      "Success Rate: 80.00%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up left up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.9, theta=0.001, episodes=100, epsilon=0\n",
      "Success Rate: 84.00%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up left up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.9, theta=1e-05, episodes=100, epsilon=0\n",
      "Success Rate: 71.00%\n",
      "\n",
      "map: \n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n",
      "\n",
      "directions: \n",
      "left up up up left left left left up down left left left right down left \n",
      "\n",
      "-------\n",
      "Hyperparameters: gamma=0.99, theta=1e-07, episodes=1000, epsilon=0\n",
      "Success Rate: 82.80%\n"
     ]
    }
   ],
   "source": [
    "# base level test\n",
    "test1 = Frozen_lake()\n",
    "print(test1.policy)\n",
    "test1.run()\n",
    "#play around with hyperparameters\n",
    "test2 = Frozen_lake(epsilon=0.1)\n",
    "test2.run()\n",
    "test3 = Frozen_lake(episodes=1000,epsilon=0.1)\n",
    "test3.run()\n",
    "test4 = Frozen_lake(episodes=1000,epsilon=0.2)\n",
    "test4.run()\n",
    "test5=Frozen_lake(episodes=100, theta = 1e-5)\n",
    "test5.run()\n",
    "test6 = Frozen_lake(episodes=100, gamma = 0.9)\n",
    "test6.run()\n",
    "test7 = Frozen_lake(episodes=100, gamma = 0.9, theta = 1e-5)\n",
    "test7.run()\n",
    "test8=Frozen_lake(episodes=1000, theta = 1e-7)\n",
    "test8.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
